#A1
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from sklearn.impute import SimpleImputer
file_path = r"C:\Users\prabi\Downloads\Lab Session Data.xlsx"
thyroid_data = pd.read_excel(file_path, sheet_name='thyroid0387_UCI')
non_num_cols = thyroid_data.select_dtypes(exclude=[np.number]).columns
print("Non-numeric columns:", non_num_cols)
thyroid_data_num = thyroid_data.drop(columns=non_num_cols)
x = thyroid_data_num.iloc[:, :-1].values
y = thyroid_data.iloc[:, -1].values 
print("Missing values in x:", np.isnan(x).sum())
print("Missing values in y:", pd.isnull(y).sum())  # Using pd.isnull() for y
imputer = SimpleImputer(strategy='mean')
x = imputer.fit_transform(x)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train, y_train)
acc = knn.score(x_test, y_test)
print("kNN Accuracy:", acc)
preds = knn.predict(x_test)  
print("Accuracy:", accuracy_score(y_test, preds))  
print("Confusion Matrix:\n", confusion_matrix(y_test, preds))  
print("Predictions:", preds)
#--------------------------------------------------
#A2
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
file_path = r"C:\Users\prabi\Downloads\Lab Session Data.xlsx"
buying_data = pd.read_excel(file_path, sheet_name='Purchase data')
matA = buying_data[['Candies (#)', 'Mangoes (Kg)', 'Milk Packets (#)']].dropna().values
matC = buying_data['Payment (Rs)'].dropna().values
A_pseudo_inv = np.linalg.pinv(matA)
matX = A_pseudo_inv @ matC
predictions = matA @ matX
mse = mean_squared_error(matC, predictions)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((matC - predictions) / matC)) * 100
r2 = r2_score(matC, predictions)
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"Mean Absolute Percentage Error (MAPE): {mape}%")
print(f"R-squared (R2) score: {r2}")
#-----------------------------------------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

path_of_file = r"C:\Users\prabi\OneDrive\Desktop\V sem\ML\Assignment\java_cc_embed_data.csv"
proj_dataset = pd.read_csv(path_of_file)

feature1 = proj_dataset[['cc_embedding_1', 'cc_embedding_2']].values
feature2 = proj_dataset['Final_Marks'].values  

# A3
feature1_train = feature1[(feature1[:, 0] >= 1) & (feature1[:, 0] <= 10) & (feature1[:, 1] >= 1) & (feature1[:, 1] <= 10)]
feature2_train = feature2[(feature1[:, 0] >= 1) & (feature1[:, 0] <= 10) & (feature1[:, 1] >= 1) & (feature1[:, 1] <= 10)]

plt.scatter(feature1_train[feature2_train == 0][:, 0], feature1_train[feature2_train == 0][:, 1], color='blue', label='Class 0')
plt.scatter(feature1_train[feature2_train == 1][:, 0], feature1_train[feature2_train == 1][:, 1], color='red', label='Class 1')
plt.xlabel('cc_embedding_1')
plt.ylabel('cc_embedding_2')
plt.legend()
plt.title('Training Data')
plt.show()

# A4: 
feature1_test = np.array([[x, y] for x in np.arange(1, 10.1, 0.1) for y in np.arange(1, 10.1, 0.1)])
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(feature1_train, feature2_train)
feature2_test_pred = knn.predict(feature1_test)

plt.scatter(feature1_test[feature2_test_pred == 0][:, 0], feature1_test[feature2_test_pred == 0][:, 1], color='blue', label='Class 0')
plt.scatter(feature1_test[feature2_test_pred == 1][:, 0], feature1_test[feature2_test_pred == 1][:, 1], color='red', label='Class 1')
plt.xlabel('cc_embedding_1')
plt.ylabel('cc_embedding_2')
plt.legend()
plt.title('Test Data for k=3')
plt.show()

# A5:
for k in [1, 5, 7, 9]:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(feature1_train, feature2_train)
    feature2_test_pred = knn.predict(feature1_test)

    plt.scatter(feature1_test[feature2_test_pred == 0][:, 0], feature1_test[feature2_test_pred == 0][:, 1], color='blue', label='Class 0')
    plt.scatter(feature1_test[feature2_test_pred == 1][:, 0], feature1_test[feature2_test_pred == 1][:, 1], color='red', label='Class 1')
    plt.xlabel('cc_embedding_1')
    plt.ylabel('cc_embedding_2')
    plt.legend()
    plt.title(f'Test Data for k={k}')
    plt.show()

# A6:
feature1_train = proj_dataset[['cc_embedding_3', 'cc_embedding_4']].values[:20]
feature2_train = proj_dataset['error_count'].values[:20]

# A7
param_grid = {'n_neighbors': np.arange(1, 21)}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid_search.fit(feature1_train, feature2_train)

best_k = grid_search.best_params_['n_neighbors']
print(f'Best k value found: {best_k}')

# Fit and predict with the best k value
best_knn = KNeighborsClassifier(n_neighbors=best_k)
best_knn.fit(feature1_train, feature2_train)
project_feature2_test_pred = best_knn.predict(feature1_test)

plt.scatter(feature1_test[project_feature2_test_pred == 0][:, 0], feature1_test[project_feature2_test_pred == 0][:, 1], color='blue', label='Class 0')
plt.scatter(feature1_test[project_feature2_test_pred == 1][:, 0], feature1_test[project_feature2_test_pred == 1][:, 1], color='red', label='Class 1')
plt.xlabel('cc_embedding_1')
plt.ylabel('cc_embedding_2')
plt.legend()
plt.title(f'Test Data for Best k={best_k}')
plt.show()
